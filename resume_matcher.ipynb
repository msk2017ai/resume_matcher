{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#               RESUME MATCHING PROTYPE EXPERIMENT\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  THIS APPLICATION IDENTIFIES TO HOW CLOSE THE RESUME IS MATCHING WITH THE JOB DESCRIPTION. \r\n",
    "##  WE CAN USE THIS METHODOLOGY TO IDENTIFY AND MATCH HOW EXTENT THE CANDIDATE PROFILES ARE MATCHING WITH THE JOB DESCRIPTIONS."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from docx import Document\r\n",
    "from docx.shared import Inches\r\n",
    "import nltk\r\n",
    "#nltk.download('punkt')\r\n",
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## READING JOB DESCRIPTION (SAMPLE USED FOR THIS EXPERIMENT)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "jd = Document('job_description/job_description.docx')\r\n",
    "lis = []\r\n",
    "job_description =[]\r\n",
    "for para in jd.paragraphs:\r\n",
    "    lis.append(para.text)\r\n",
    "import nltk\r\n",
    "nltk.download('punkt')\r\n",
    "print(len(lis))\r\n",
    "for i in range(len(lis)):\r\n",
    "    tokens = nltk.word_tokenize(lis[i])\r\n",
    "    job_description.append((tokens))\r\n",
    "job_description_list = ' '.join([ls for x in job_description for ls in x])\r\n",
    "print(job_description_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14\n",
      "Job Description : AI/ML - Data Scientist : Years of experience : 7- 15 Years Data Scientists with the ability to perform independent statistical and machine learning research/ projects . If you are a problem solution builder with a curiosity for exploring new techniques and technologies in AIML ( Artificial Intelligence & Machine Learning ) space . Individuals should be able to break down business problems into smaller components and implement ML approaches to empower the end business decisions Key Responsibilities Intermediate to expert level proficiency in statistical/ML predictive techniques such as regression , Bayesian methods , tree-based learners , SVM , RF , XGBOOST etc Intermediate to expert level proficiency and thorough understanding of at least one of the upcoming technologies - Deep learning & Computer Vision Working experience and statistical clarity in traditional algorithms like time series modeling , dimensionality reduction , SEM , GLM , GLMM , clustering and related areas Experience of working on a project end-to-end : data gathering , EDA , visualizations , modeling and insights , problem scoping . Collaborate with business and data owners to formulate problems whose solution would be impactful to the business Strong hands on skill in Python using libraries like Seaborn , NLTK/Spacy , SkLearn , Keras/Tensorflow , Fastai/Pytorch , Pandas , Matplotlib , etc Good to have Experience working with large data sets and tools like MapReduce , Hadoop , Hive , etc and programming algorithms that are currently not available in distributed environment . Good to have experience in any one of the cloud platforms such as AWS , GCP or Azure\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sures\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## READING SAMPLE RESUME - A"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\r\n",
    "document_a = Document('sample_resumes/Resume_A.docx')\r\n",
    "lis = []\r\n",
    "textual_a =[]\r\n",
    "for para in document_a.paragraphs:\r\n",
    "    lis.append(para.text)\r\n",
    "\r\n",
    "print(len(lis))\r\n",
    "for i in range(len(lis)):\r\n",
    "    tokens = nltk.word_tokenize(lis[i])\r\n",
    "    textual_a.append((tokens))\r\n",
    "textual_a_list = ' '.join([ls for x in textual_a for ls in x])\r\n",
    "print(textual_a_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19\n",
      "KAMAL H My Skills are Jupyter Notebook ML FLOW Anaconda Navigator Matplotlib PYSPARK AGILE Methodology Regression Bayesian Methods Tree-based learners SVM RF XGBOOST\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## READING SAMPLE RESUME - B"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from docx import Document\r\n",
    "from docx.shared import Inches\r\n",
    "document_b = Document('sample_resumes/Resume_B.docx')\r\n",
    "lis = []\r\n",
    "textual_b =[]\r\n",
    "for para in document_b.paragraphs:\r\n",
    "    lis.append(para.text)\r\n",
    "print(len(lis))\r\n",
    "for i in range(len(lis)):\r\n",
    "    tokens = nltk.word_tokenize(lis[i])\r\n",
    "    textual_b.append((tokens))\r\n",
    "textual_b_list = ' '.join([ls for x in textual_b for ls in x])\r\n",
    "print(textual_b_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "13\n",
      "RAJINI K My Skills are IBM Mainframe DB2 CICS VSAM JCL XGBOOST SVM\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IDENTIFY AND MATCH RESUME A TO HOW EXTENT THIS MATCHES WITH CURRENT SAMPLE JOB DESCRIPTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# tokenization\r\n",
    "X_list = word_tokenize(textual_a_list) \r\n",
    "Y_list = word_tokenize(job_description_list)\r\n",
    "  \r\n",
    "# sw contains the list of stopwords\r\n",
    "sw = stopwords.words('english') \r\n",
    "l1 =[];l2 =[]\r\n",
    "X_set = {w for w in X_list if not w in sw} \r\n",
    "Y_set = {w for w in Y_list if not w in sw}\r\n",
    "  \r\n",
    "# form a set containing keywords of both strings \r\n",
    "rvector = X_set.union(Y_set) \r\n",
    "for w in rvector:\r\n",
    "    if w in X_set: l1.append(1) # create a vector\r\n",
    "    else: l1.append(0)\r\n",
    "    if w in Y_set: l2.append(1)\r\n",
    "    else: l2.append(0)\r\n",
    "c = 0\r\n",
    "  \r\n",
    "# cosine formula \r\n",
    "for i in range(len(rvector)):\r\n",
    "        c+= l1[i]*l2[i]\r\n",
    "cosine = c / float((sum(l1)*sum(l2))**0.5)\r\n",
    "print(\"similarity: \", cosine)\r\n",
    "if cosine > 0.1:\r\n",
    "    print(\"THIS RESUME MATCHES WTH JOB REQUIREMENT WITH A SCORE OF = \", cosine)\r\n",
    "else:\r\n",
    "    print(\"THIS RESUME DOES NOT MATCH WTH JOB REQUIREMENT, LOW SCORE OF = \", cosine)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "similarity:  0.12613124477737825\n",
      "THIS RESUME MATCHES WTH JOB REQUIREMENT WITH A SCORE OF =  0.12613124477737825\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IDENTIFY AND MATCH RESUME B TO HOW EXTENT THIS MATCHES WITH CURRENT SAMPLE JOB DESCRIPTION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# tokenization\r\n",
    "X_list = word_tokenize(textual_b_list) \r\n",
    "Y_list = word_tokenize(job_description_list)\r\n",
    "  \r\n",
    "# sw contains the list of stopwords\r\n",
    "sw = stopwords.words('english') \r\n",
    "l1 =[];l2 =[]\r\n",
    "X_set = {w for w in X_list if not w in sw} \r\n",
    "Y_set = {w for w in Y_list if not w in sw}\r\n",
    "  \r\n",
    "# form a set containing keywords of both strings \r\n",
    "rvector = X_set.union(Y_set) \r\n",
    "for w in rvector:\r\n",
    "    if w in X_set: l1.append(1) # create a vector\r\n",
    "    else: l1.append(0)\r\n",
    "    if w in Y_set: l2.append(1)\r\n",
    "    else: l2.append(0)\r\n",
    "c = 0\r\n",
    "  \r\n",
    "# cosine formula \r\n",
    "for i in range(len(rvector)):\r\n",
    "        c+= l1[i]*l2[i]\r\n",
    "cosine = c / float((sum(l1)*sum(l2))**0.5)\r\n",
    "print(\"similarity: \", cosine)\r\n",
    "if cosine > 0.1:\r\n",
    "    print(\"THIS RESUME MATCHES WTH JOB REQUIREMENT WITH A SCORE OF = \", cosine)\r\n",
    "else:\r\n",
    "    print(\"THIS RESUME DOES NOT MATCH WTH JOB REQUIREMENT, LOW SCORE OF = \", cosine)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "similarity:  0.048795003647426664\n",
      "THIS RESUME DOES NOT MATCH WTH JOB REQUIREMENT, LOW SCORE OF =  0.048795003647426664\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3df61ff138e1a313d712cdd23a8996aca41e2db4b95c43665eb9977f7d39f071"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}